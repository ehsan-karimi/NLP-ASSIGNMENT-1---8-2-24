{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/iyvbgx2o7vv9QRYPZKrt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II5eSTNIWqLj",
        "outputId": "2892f3d8-2f4a-4a6c-f0ed-ea12881651c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Wikipedia pages...\n",
            "Preprocessing texts with SpaCy...\n",
            "\n",
            "=== Naive Bayes ===\n",
            "Confusion Matrix:\n",
            "[[3 1]\n",
            " [0 2]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    geographic       1.00      0.75      0.86         4\n",
            "non-geographic       0.67      1.00      0.80         2\n",
            "\n",
            "      accuracy                           0.83         6\n",
            "     macro avg       0.83      0.88      0.83         6\n",
            "  weighted avg       0.89      0.83      0.84         6\n",
            "\n",
            "\n",
            "=== Logistic Regression ===\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [1 1]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    geographic       0.80      1.00      0.89         4\n",
            "non-geographic       1.00      0.50      0.67         2\n",
            "\n",
            "      accuracy                           0.83         6\n",
            "     macro avg       0.90      0.75      0.78         6\n",
            "  weighted avg       0.87      0.83      0.81         6\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import wikipedia\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1. Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 2. Define page titles\n",
        "geographic_titles = [\n",
        "    \"Iran\", \"Africa\", \"Amazon River\", \"Rome\", \"Iraq\",\n",
        "    \"Nile\", \"Paris\", \"Tokyo\", \"Milan\", \"Australia\"\n",
        "]\n",
        "\n",
        "non_geographic_titles = [\n",
        "    \"Artificial intelligence\", \"Philosophy\", \"Quantum mechanics\",\n",
        "    \"Basketball\", \"Education\", \"Deep learning\",\n",
        "    \"Python (programming language)\", \"Photosynthesis\",\n",
        "    \"Human rights\", \"Blockchain\"\n",
        "]\n",
        "\n",
        "# 3. Preprocessing using SpaCy\n",
        "def preprocess_with_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# 4. Fetch Wikipedia pages\n",
        "def fetch_wikipedia_text(titles):\n",
        "    documents = []\n",
        "    for title in titles:\n",
        "        try:\n",
        "            page = wikipedia.page(title)\n",
        "            documents.append(page.content)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch {title}: {e}\")\n",
        "    return documents\n",
        "\n",
        "print(\"Fetching Wikipedia pages...\")\n",
        "geo_docs = fetch_wikipedia_text(geographic_titles)\n",
        "non_geo_docs = fetch_wikipedia_text(non_geographic_titles)\n",
        "\n",
        "# 5. Prepare dataset\n",
        "X_raw = geo_docs + non_geo_docs\n",
        "y = ['geographic'] * len(geo_docs) + ['non-geographic'] * len(non_geo_docs)\n",
        "\n",
        "print(\"Preprocessing texts with SpaCy...\")\n",
        "X_cleaned = [preprocess_with_spacy(doc) for doc in X_raw]\n",
        "\n",
        "# 6. Feature extraction (Bag of Words)\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X_features = vectorizer.fit_transform(X_cleaned)\n",
        "\n",
        "# 7. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 8. Train Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "\n",
        "# 9. Train Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "\n",
        "# 10. Evaluate results\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "evaluate_model(\"Naive Bayes\", y_test, nb_preds)\n",
        "evaluate_model(\"Logistic Regression\", y_test, lr_preds)\n"
      ]
    }
  ]
}